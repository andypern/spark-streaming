/** note: this only appears to work if you start your netcat listener prior to launching, eg:
nc -lk 9999.  But, to make things more useful, do something like this:
1) mkfifo /mapr/cluster/input
2) tail -f /mapr/cluster/input | nc -lk 9999
3) start this spark app to connect to the port
4) echo/cat stuff into /mapr/cluster/input
This should allow the connection to stay open, and be able to shove whatever you want into the FIFO and have it show up on the spark side.
*/

/*the idea here is that you'll have a netcat stream of CSV data coming in, and we need to:
1. insert/append the row into M7-tables
2. write/append the CSV to disk, for now just using loopback NFS.
*/

/*TODO:
- figure out how to get each line as a separte element...then each field as a separate element
- find way for scala to talk to M7..or perhaps just find a way to use mapr HBASE JAVA class here?
- decide how to chunk output files up..eg: every minute? hour? day? # of rows?
- read this : https://spark-project.atlassian.net/browse/SPARK-944
*/


package org.apache.spark.streaming.m7import

import java.io._
import scala.io.Source
import scala.util.Random
import org.apache.spark._
//import org.apache.spark.rdd
import org.apache.spark.rdd.RDD
import org.apache.spark.SparkContext._
import org.apache.spark.streaming._
import org.apache.spark.streaming.StreamingContext._
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.storage.StorageLevel
import org.apache.spark.util.IntParam



/** probably better to package the logging function up as a separate class, but for now this is fine */

import org.apache.spark.Logging
import org.apache.log4j.{Level, Logger}

/** Utility functions for Spark Streaming examples. */
object StreamingExamples extends Logging {

  /** Set reasonable logging levels for streaming if the user has not configured log4j. */
  def setStreamingLogLevels() {
    val log4jInitialized = Logger.getRootLogger.getAllAppenders.hasMoreElements
    if (!log4jInitialized) {
      // We first log something to initialize Spark's default logging, then we override the
      // logging level.
      logInfo("Setting log level to [WARN] for streaming example." +
        " To override add a custom log4j.properties to the classpath.")
      Logger.getRootLogger.setLevel(Level.WARN)
    }
    Logger.getRootLogger.setLevel(Level.WARN)
  }
}


/** now to the actual work..borrowed from spark streaming examples */

object m7import {
  def main(args: Array[String]) {
    if (args.length < 4) {
      System.err.println("Usage: m7import <master> <hostname> <port> <batchsecs>\n" +
        "In local mode, <master> should be 'local[n]' with n > 1")
      System.exit(1)
    }

  
    StreamingExamples.setStreamingLogLevels()

     val Array(master, host, IntParam(port), IntParam(batchsecs)) = args


    // Create the context with a X second batch size, where X is the arg you supplied as 'batchsecs'.
    val ssc = new StreamingContext(master, "NetworkWordCount", Seconds(batchsecs),
      System.getenv("SPARK_HOME"), StreamingContext.jarOfClass(this.getClass))



    //not needed, since we can get it via the arg's.

    //val lines = ssc.socketTextStream(192.168.6.135, 9999)



    // Create a NetworkInputDStream on target ip:port and count the
    // words in input stream of \n delimited text (eg. generated by 'nc')
    val records = ssc.socketTextStream(host, port.toInt, StorageLevel.MEMORY_ONLY_SER)

    
    //unnecessary?: split each line into fields, delimited by ,
   // val lines = records.flatMap(_.split("\n"))


    //not needed for this excercise.  split each line into fields, delimited by ,
    //val words = records.flatMap(_.split(","))


    //basically, foreach rdd inside the Dstream, perform a 'collect' on the RDD, which creates an array, 
    // and run a foreach on the elements within the array.  Maybe there's a more 'sparky' way of doing this..so sue me.
    records.foreach(rdd => {
      val rddarray = rdd.collect
        if(rddarray.length > 0) {
          for(line <- rddarray) {
             //time to split this row into words, from scala-cookbook, the .trim removes leading/trailing
             //spaces from the values.
            val Array(resID, date, time, hz, disp, flo, sedPPM, psi, chlPPM) = line.split(",").map(_.trim)
            //now we need some code to shove into m7..

            println(s"Writing this to m7: $resID,$date,$time,$hz,$disp,$flo,$sedPPM,$psi,$chlPPM")
          }
          //now that each of the rows are in m7 , lets dump the entire RDD to disk.
          rdd.saveAsTextFile("/mapr/shark/CSV/" + Random.nextInt(Integer.MAX_VALUE))
         
          //needless println
          //println("record array length: " + rddarray.length + " first row: " + rddarray(0))
        }
    })




    // print lines to console
    //records.print()
    ssc.start()             // Start the computation
    ssc.awaitTermination()  // Wait for the computation to

  }
}